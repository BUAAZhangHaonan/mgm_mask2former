# smoke_test_mgm.py
# -*- coding: utf-8 -*-
import argparse
import random
from typing import Dict

import torch
import torch.nn as nn

from detectron2.config import get_cfg
from detectron2.data import DatasetCatalog, MetadataCatalog
from detectron2.structures import BitMasks, Instances

from mask2former.modeling.config.mgm_config import add_mgm_config
from mask2former.modeling.meta_arch.mgm_model import MGMMaskFormer


def count_parameters(model: nn.Module, detailed: bool = True) -> Dict:
    """
    ç»Ÿè®¡æ¨¡å‹å‚æ•°çš„è¯¦ç»†ä¿¡æ¯

    Args:
        model: PyTorchæ¨¡å‹
        detailed: æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„å­æ¨¡å—å‚æ•°ç»Ÿè®¡

    Returns:
        åŒ…å«å‚æ•°ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    total_params = 0
    trainable_params = 0
    frozen_params = 0

    # ç»Ÿè®¡å„ä¸ªå­æ¨¡å—çš„å‚æ•°
    module_stats = {}

    def format_number(num):
        """æ ¼å¼åŒ–æ•°å­—ï¼Œæ·»åŠ åƒä½åˆ†éš”ç¬¦"""
        return f"{num:,}"

    def get_size_mb(num_params):
        """è®¡ç®—å‚æ•°å ç”¨çš„å†…å­˜å¤§å°ï¼ˆå‡è®¾float32ï¼Œæ¯ä¸ªå‚æ•°4å­—èŠ‚ï¼‰"""
        return num_params * 4 / (1024 * 1024)

    print("=" * 80)
    print("æ¨¡å‹å‚æ•°ç»Ÿè®¡")
    print("=" * 80)

    # ç»Ÿè®¡æ€»å‚æ•°
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count

        if param.requires_grad:
            trainable_params += param_count
        else:
            frozen_params += param_count

        # æŒ‰æ¨¡å—å½’ç±»ç»Ÿè®¡
        module_name = name.split(".")[0] if "." in name else "root"
        if module_name not in module_stats:
            module_stats[module_name] = {
                "total": 0,
                "trainable": 0,
                "frozen": 0,
                "params": [],
            }

        module_stats[module_name]["total"] += param_count
        if param.requires_grad:
            module_stats[module_name]["trainable"] += param_count
        else:
            module_stats[module_name]["frozen"] += param_count

        module_stats[module_name]["params"].append(
            {
                "name": name,
                "shape": list(param.shape),
                "params": param_count,
                "trainable": param.requires_grad,
            }
        )

    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    print(f"æ€»å‚æ•°æ•°é‡: {format_number(total_params)}")
    print(f"å¯è®­ç»ƒå‚æ•°: {format_number(trainable_params)}")
    print(f"å†»ç»“å‚æ•°: {format_number(frozen_params)}")
    print(f"æ¨¡å‹å¤§å°: {get_size_mb(total_params):.2f} MB")
    print(f"å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {trainable_params/total_params*100:.2f}%")

    if detailed:
        print("\n" + "=" * 80)
        print("å„æ¨¡å—å‚æ•°ç»Ÿè®¡")
        print("=" * 80)

        # æŒ‰å‚æ•°æ•°é‡æ’åº
        sorted_modules = sorted(
            module_stats.items(), key=lambda x: x[1]["total"], reverse=True
        )

        for module_name, stats in sorted_modules:
            print(f"\nğŸ“¦ {module_name}:")
            print(
                f"  æ€»å‚æ•°: {format_number(stats['total'])} ({stats['total']/total_params*100:.2f}%)"
            )
            print(f"  å¯è®­ç»ƒ: {format_number(stats['trainable'])}")
            print(f"  å†»ç»“: {format_number(stats['frozen'])}")
            print(f"  å¤§å°: {get_size_mb(stats['total']):.2f} MB")

            # æ˜¾ç¤ºè¯¥æ¨¡å—ä¸‹çš„ä¸»è¦å‚æ•°å±‚
            large_params = [p for p in stats["params"] if p["params"] > 1000]
            large_params.sort(key=lambda x: x["params"], reverse=True)

            if large_params:
                print(f"  ä¸»è¦å‚æ•°å±‚ (>1Kå‚æ•°):")
                for param_info in large_params[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæœ€å¤§çš„
                    trainable_mark = "âœ“" if param_info["trainable"] else "âœ—"
                    print(
                        f"    {trainable_mark} {param_info['name']}: {param_info['shape']} -> {format_number(param_info['params'])}"
                    )
                if len(large_params) > 5:
                    print(f"    ... è¿˜æœ‰ {len(large_params) - 5} ä¸ªå‚æ•°å±‚")

    print("\n" + "=" * 80)
    print("æ¨¡å‹ç»“æ„æ¦‚è§ˆ")
    print("=" * 80)

    # æ‰“å°æ¨¡å‹çš„ä¸»è¦ç»“æ„
    def print_model_structure(model, prefix="", max_depth=2, current_depth=0):
        if current_depth >= max_depth:
            return

        for name, child in model.named_children():
            child_params = sum(p.numel() for p in child.parameters())
            if child_params > 0:
                print(
                    f"{prefix}â”œâ”€ {name}: {child.__class__.__name__} ({format_number(child_params)} params)"
                )
                if current_depth < max_depth - 1:
                    print_model_structure(
                        child, prefix + "â”‚  ", max_depth, current_depth + 1
                    )

    print_model_structure(model)

    # è¿”å›ç»Ÿè®¡ç»“æœ
    result = {
        "total_params": total_params,
        "trainable_params": trainable_params,
        "frozen_params": frozen_params,
        "model_size_mb": get_size_mb(total_params),
        "trainable_ratio": trainable_params / total_params,
        "module_stats": module_stats,
    }

    return result


def register_dummy_dataset(name: str, num_thing_classes: int = 3):
    # ç©ºæ•°æ®é›†å ä½å³å¯ï¼›å…³é”®æ˜¯ç»™ meta éœ€è¦çš„æ˜ å°„
    if name in DatasetCatalog.list():
        return
    DatasetCatalog.register(name, lambda: [])
    # è®¾ç½® thing æ˜ å°„ï¼ˆä¾› instance / panoptic æ¨ç†è·¯å¾„ä½¿ç”¨ï¼‰
    thing_map = {i: i for i in range(num_thing_classes)}
    MetadataCatalog.get(name).set(
        thing_dataset_id_to_contiguous_id=thing_map,
        thing_classes=[f"cls{i}" for i in range(num_thing_classes)],
    )


def build_cfg(yaml_path: str, device: str = None):
    cfg = get_cfg()
    add_mgm_config(cfg)
    cfg.merge_from_file(yaml_path)

    # è¦†ç›–æˆ dummy æ•°æ®é›†ï¼Œé¿å…ä¾èµ–çœŸå®æ³¨å†Œ
    train_name = "__dummy_rgbd_train__"
    test_name = "__dummy_rgbd_test__"
    register_dummy_dataset(
        train_name, num_thing_classes=cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES
    )
    register_dummy_dataset(
        test_name, num_thing_classes=cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES
    )
    cfg.DATASETS.TRAIN = (train_name,)
    cfg.DATASETS.TEST = (test_name,)

    # æ¨ç†åªå¼€ instanceï¼Œå…³æ‰ semantic/panoptic å¯å‡å°‘æ— å…³åˆ†æ”¯
    cfg.MODEL.MASK_FORMER.TEST.SEMANTIC_ON = False
    cfg.MODEL.MASK_FORMER.TEST.PANOPTIC_ON = False
    cfg.MODEL.MASK_FORMER.TEST.INSTANCE_ON = True

    # é¿å…æƒé‡åŠ è½½
    cfg.MODEL.WEIGHTS = ""

    # è®¾å¤‡
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    cfg.MODEL.DEVICE = device
    return cfg


def make_random_instances(h, w, num_classes, max_inst=2):
    """æ„é€ éšæœº Instancesï¼ˆBitMasks + gt_classesï¼‰"""
    n = random.randint(1, max_inst)
    masks = torch.zeros((n, h, w), dtype=torch.bool)
    for i in range(n):
        # éšæœºçŸ©å½¢ä½œä¸º mask
        rh, rw = random.randint(h // 8, h // 3), random.randint(w // 8, w // 3)
        y0 = random.randint(0, max(0, h - rh))
        x0 = random.randint(0, max(0, w - rw))
        masks[i, y0 : y0 + rh, x0 : x0 + rw] = True
    gt = Instances((h, w))
    gt.gt_masks = BitMasks(masks)
    gt.gt_classes = torch.randint(low=0, high=num_classes, size=(n,), dtype=torch.int64)
    return gt


def make_fake_batch(
    B=2, H=256, W=256, with_targets=True, with_noise_mask=True, device="cpu"
):
    """
    æ„é€ ä¸€ä¸ª batched_inputs åˆ—è¡¨ï¼Œå…ƒç´  shape:
      - image: [3,H,W] uint8
      - depth: [1,H,W] float32 in [0,1]
      - depth_noise_mask: [1,H,W] float32 (å¯é€‰)
      - instances: detectron2 Instances (è®­ç»ƒæ—¶å¿…å¤‡)
      - height/width: åŸå§‹å°ºå¯¸ï¼ˆä¾›åå¤„ç†ï¼‰
    """
    batch = []
    for _ in range(B):
        img = torch.randint(0, 256, (3, H, W), dtype=torch.uint8, device=device)
        depth = torch.rand(1, H, W, dtype=torch.float32, device=device)
        sample = {
            "image": img,
            "depth": depth,
            "height": H,
            "width": W,
        }
        if with_noise_mask:
            # æ³¨æ„ï¼šè¿™é‡Œå·²ç»æ˜¯ CHWï¼ˆ1Ã—HÃ—Wï¼‰ï¼Œä¸è¦å† unsqueeze
            noise = (torch.rand(1, H, W, device=device) > 0.5).float()
            sample["depth_noise_mask"] = noise
        if with_targets:
            # num_classes åœ¨æ„å›¾åä» cfg é‡Œæ‹¿ï¼›è¿™é‡Œå…ˆå ä½ï¼Œç¨åè¡¥
            sample["_need_targets"] = True  # å ä½æ ‡è®°
        batch.append(sample)
    return batch


def attach_targets(batch, num_classes):
    for s in batch:
        if s.pop("_need_targets", False):
            H, W = s["height"], s["width"]
            s["instances"] = make_random_instances(H, W, num_classes)
    return batch


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--cfg", required=True, help="Path to mgm_swin_convnext_tiny.yaml"
    )
    parser.add_argument("--device", default=None, help="cuda or cpu")
    parser.add_argument("--size", type=int, default=256)
    parser.add_argument("--batch", type=int, default=2)
    parser.add_argument(
        "--detailed-stats", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†çš„æ¨¡å‹å‚æ•°ç»Ÿè®¡ä¿¡æ¯"
    )
    args = parser.parse_args()

    cfg = build_cfg(args.cfg, device=args.device)
    device = cfg.MODEL.DEVICE
    H = W = args.size
    B = args.batch

    # æ„å»ºæ¨¡å‹
    model_kwargs = MGMMaskFormer.from_config(cfg)
    model = MGMMaskFormer(**model_kwargs).to(device)

    # ç»Ÿè®¡æ¨¡å‹å‚æ•°
    print(f"\nğŸ” æ­£åœ¨åˆ†ææ¨¡å‹å‚æ•°...")
    param_stats = count_parameters(model, detailed=args.detailed_stats)

    model.train()

    # è®­ç»ƒè·¯å¾„ï¼šæ„é€  batchï¼ˆå« targetsï¼‰
    batched_inputs = make_fake_batch(
        B=B, H=H, W=W, with_targets=True, with_noise_mask=True, device=device
    )
    batched_inputs = attach_targets(
        batched_inputs, num_classes=cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES
    )

    # å‰å‘ + åä¼ ï¼ˆåªåšä¸€æ¬¡ï¼ŒéªŒè¯æ¢¯åº¦é“¾è·¯ï¼‰
    losses = model(batched_inputs)
    loss_sum = sum(v for v in losses.values())
    print("[train] losses:", {k: float(v.detach().cpu()) for k, v in losses.items()})
    loss_sum.backward()
    print("[train] backward ok, total loss =", float(loss_sum.detach().cpu()))

    # æ¨ç†è·¯å¾„ï¼šå»æ‰ targets
    model.eval()
    infer_inputs = make_fake_batch(
        B=B, H=H, W=W, with_targets=False, with_noise_mask=True, device=device
    )
    with torch.no_grad():
        outputs = model(infer_inputs)
    print("[eval] num outputs:", len(outputs))
    for i, out in enumerate(outputs):
        keys = list(out.keys())
        print(f"[eval] sample {i} keys:", keys)
        if "instances" in out:
            inst = out["instances"]
            print(
                f"  instances: {len(inst)} masks={getattr(inst, 'pred_masks', None) is not None}"
            )

    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆ - å‚æ•°ç»Ÿè®¡æ‘˜è¦")
    print("=" * 80)
    print(f"âœ… æ¨¡å‹æ€»å‚æ•°: {param_stats['total_params']:,}")
    print(f"âœ… å¯è®­ç»ƒå‚æ•°: {param_stats['trainable_params']:,}")
    print(f"âœ… æ¨¡å‹å¤§å°: {param_stats['model_size_mb']:.2f} MB")
    print(f"âœ… è®­ç»ƒ/æ¨ç†æµ‹è¯•: é€šè¿‡")
    print("Smoke test passed âœ”")


if __name__ == "__main__":
    main()
